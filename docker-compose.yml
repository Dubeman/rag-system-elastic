version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: rag-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - cluster.name=rag-cluster
      - node.name=rag-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  elasticsearch-setup:
    image: curlimages/curl:latest
    container_name: rag-elasticsearch-setup
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - rag-network
    command: |
      sh -c "
        echo 'Waiting for Elasticsearch to be fully ready...'
        sleep 15
        
        echo 'Step 1: Checking if ELSER model exists...'
        if curl -s 'http://elasticsearch:9200/_ml/trained_models/.elser_model_2' | grep -q '.elser_model_2'; then
          echo '‚úÖ ELSER model already exists!'
        else
          echo '‚ùå ELSER model not found - this should not happen'
          exit 1
        fi
        
        echo 'Step 2: Creating ELSER inference pipeline...'
        PIPELINE_RESPONSE=\$(curl -s -X PUT 'http://elasticsearch:9200/_ingest/pipeline/elser_pipeline' -H 'Content-Type: application/json' -d '{\"description\": \"ELSER v2 inference pipeline\", \"processors\": [{\"inference\": {\"model_id\": \".elser_model_2\", \"target_field\": \"text_expansion\", \"field_map\": {\"text\": \"text_field\"}}}]}' 2>/dev/null || echo 'pipeline_creation_failed')
        
        if echo \$PIPELINE_RESPONSE | grep -q 'acknowledged.*true'; then
          echo '‚úÖ ELSER inference pipeline created successfully!'
        else
          echo '‚ùå Failed to create ELSER pipeline: ' \$PIPELINE_RESPONSE
        fi
        
        echo 'Step 3: Testing ELSER inference pipeline...'
        sleep 10
        
        TEST_RESPONSE=\$(curl -s -X POST 'http://elasticsearch:9200/_ingest/pipeline/elser_pipeline/_simulate' -H 'Content-Type: application/json' -d '{\"docs\": [{\"_source\": {\"text\": \"This is a test document for ELSER inference.\"}}]}' 2>/dev/null || echo 'pipeline_test_failed')
        
        if echo \$TEST_RESPONSE | grep -q 'text_expansion'; then
          echo '‚úÖ ELSER inference pipeline is working!'
          echo 'Sample inference result:'
          echo \$TEST_RESPONSE | jq '.docs[0]._source.text_expansion' 2>/dev/null || echo 'Could not parse result'
        else
          echo '‚ùå ELSER inference pipeline test failed: ' \$TEST_RESPONSE
        fi
        
        echo 'Step 4: Verifying pipeline exists...'
        PIPELINE_CHECK=\$(curl -s 'http://elasticsearch:9200/_ingest/pipeline/elser_pipeline' 2>/dev/null || echo 'pipeline_check_failed')
        
        if echo \$PIPELINE_CHECK | grep -q 'elser_pipeline'; then
          echo '‚úÖ ELSER pipeline verification successful!'
        else
          echo '‚ùå Pipeline verification failed: ' \$PIPELINE_CHECK
        fi
        
        echo 'üéâ ELSER v2 setup completed successfully!'
      "
    restart: "no"

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: rag-api
    ports:
      - "8000:8000"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - LLM_SERVICE_URL=http://ollama:11434
      - ENVIRONMENT=development
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    depends_on:
      elasticsearch:
        condition: service_healthy
      elasticsearch-setup:
        condition: service_completed_successfully
      # Temporarily disable Ollama dependency to test ELSER
      # ollama:
      #   condition: service_healthy
      # ollama-setup:
      #   condition: service_completed_successfully
    networks:
      - rag-network
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

  ui:
    build:
      context: .
      dockerfile: docker/Dockerfile.ui
    container_name: rag-ui
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
    volumes:
      - ./src:/app/src
    depends_on:
      - api
    networks:
      - rag-network
    command: streamlit run src/ui/app.py --server.port=8501 --server.address=0.0.0.0

  ollama:
    image: ollama/ollama:latest
    container_name: rag-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
    networks:
      - rag-network
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    #   start_period: 30s
    # deploy:
    #   resources:
    #     limits:
    #       memory: 6G
    #     reservations:
    #       memory: 4G

  ollama-setup:
    image: curlimages/curl:latest
    container_name: rag-ollama-setup
    # depends_on:
    #   ollama:
    #     condition: service_healthy
    networks:
      - rag-network
    command: |
      sh -c "
        echo 'Waiting for Ollama to be fully ready...'
        sleep 15
        
        echo 'Checking if tinyllama model exists...'
        if curl -s 'http://ollama:11434/api/tags' | grep -q 'tinyllama'; then
          echo 'tinyllama model already exists!'
        else
          echo 'Pulling tinyllama model...'
          curl -X POST 'http://ollama:11434/api/pull' \
            -H 'Content-Type: application/json' \
            -d '{\"name\": \"tinyllama\"}'
          echo 'tinyllama model pull initiated!'
        fi
        
        echo 'Checking available models...'
        curl -s 'http://ollama:11434/api/tags' || echo 'Model check failed'
        
        echo 'Ollama setup completed!'
      "
    restart: "no"

volumes:
  elasticsearch_data:
    driver: local
  ollama_data:
    driver: local

networks:
  rag-network:
    driver: bridge
